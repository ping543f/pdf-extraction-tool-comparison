{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f86ccf8a325c2fb15e4b7f31efc4a49871897a8af714dd6a44910958bb4a41c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Imports Successful!\n"
     ]
    }
   ],
   "source": [
    "## All imports\n",
    "from io import StringIO\n",
    "import tika\n",
    "tika.initVM()\n",
    "from tika import parser\n",
    "from PyPDF2 import PdfFileReader\n",
    "import fitz\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import pdftotext\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "from spacy.lang.en import English\n",
    "\n",
    "\n",
    "nlp = English()\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "nlp.add_pipe(sbd)\n",
    "\n",
    "print (\"Imports Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text cleaner\n",
    "def clean_sent(sent):\n",
    "    sent = re.sub(r\"\\n\",\" \",sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All pdf extraction definition\n",
    "\n",
    "#tika\n",
    "def convert_with_tika(file_name):\n",
    "    file_handle = file_path+\"/\"+file_name\n",
    "    file_writer = open(\"output_text/tika_\"+file_name+\".txt\",\"a+\",encoding=\"utf-8\")\n",
    "    parsed = parser.from_file(file_handle)\n",
    "    file_writer.write(parsed[\"content\"])\n",
    "    file_writer.close()\n",
    "    print(\"processed file: \",file_name)\n",
    "    \n",
    "\n",
    "#pypdf2\n",
    "def convert_with_pypdf2(file_name):\n",
    "    file_handle = file_path+\"/\"+file_name\n",
    "    file_writer = open(\"output_text/pypdf2_\"+file_name+\".txt\",\"a+\",encoding=\"utf-8\")\n",
    "    pdf = PdfFileReader(file_handle)\n",
    "    pages = pdf.getNumPages()\n",
    "    for page in range(pages):\n",
    "        page_content = pdf.getPage(page)\n",
    "        file_writer.write(page_content.extractText())\n",
    "    file_writer.close()\n",
    "    print(\"processed file: \",file_name)\n",
    "\n",
    "#pymupdf\n",
    "def convert_with_pymupdf(file_name):\n",
    "    file_handle = file_path+\"/\"+file_name\n",
    "    file_writer = open(\"output_text/pymupdf_\"+file_name+\".txt\",\"a+\",encoding=\"utf-8\")\n",
    "    doc = fitz.open(file_handle)\n",
    "    pages = doc.pageCount\n",
    "    for page in range(pages):\n",
    "        page_content = doc.loadPage(page)\n",
    "        file_writer.write(page_content.getText(\"text\"))\n",
    "    file_writer.close()\n",
    "    print(\"processed file: \",file_name)\n",
    "\n",
    "#pdfminer.six\n",
    "def convert_with_pdfminer(file_name):\n",
    "    file_handle = file_path+\"/\"+file_name\n",
    "    file_writer = open(\"output_text/pdfminer_\"+file_name+\".txt\",\"a+\",encoding=\"utf-8\")\n",
    "    output_string = StringIO()\n",
    "    with open(file_handle,'rb') as inp_file:\n",
    "        parser = PDFParser(inp_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    pdf_content = output_string.getvalue()\n",
    "    file_writer.write(pdf_content)\n",
    "    file_writer.close()\n",
    "    print(\"processed file: \",file_name)\n",
    "\n",
    "#pdftotext\n",
    "def convert_with_pdftotext(file_name):\n",
    "    file_handle = file_path+\"/\"+file_name\n",
    "    file_writer = open(\"output_text/pdftotext_\"+file_name+\".txt\",\"a+\",encoding=\"utf-8\")\n",
    "    with open(file_handle,'rb') as inp_file:\n",
    "        pdf = pdftotext.PDF(inp_file)\n",
    "    file_writer.write(\"\\n\\n\".join(pdf))\n",
    "    file_writer.close()\n",
    "    print(\"processed file: \",file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All sentence boundary analysis definitions\n",
    "\n",
    "#NLTK method\n",
    "def get_sent_by_nltk(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "#Spacy method\n",
    "def get_sent_by_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    sent_list = []\n",
    "    for sent in doc.sents:\n",
    "        sent_list.append(sent.text)\n",
    "    return sent_list\n",
    "\n",
    "#gensim method\n",
    "def get_sent_by_gensim(text):\n",
    "    return split_sentences(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File folder browser and file lister from a folder\n",
    "\n",
    "file_path = \"dataset\"\n",
    "def get_filenames(file_extension):\n",
    "    files = []\n",
    "    for r,d, f in os.walk(file_path+\"/\"):\n",
    "        for file in f:\n",
    "            if \".\"+file_extension in file:\n",
    "                files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 159.46 MiB, increment: -0.30 MiB\n",
      "processed file:  9.pdf\n",
      "processed file:  8.pdf\n",
      "processed file:  10.pdf\n",
      "processed file:  6.pdf\n",
      "processed file:  7.pdf\n",
      "processed file:  5.pdf\n",
      "processed file:  4.pdf\n",
      "processed file:  1.pdf\n",
      "processed file:  3.pdf\n",
      "processed file:  2.pdf\n",
      "CPU times: user 258 ms, sys: 152 ms, total: 410 ms\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reload_ext memory_profiler\n",
    "%memit\n",
    "\n",
    "files_list = get_filenames(\"pdf\")\n",
    "for filen in files_list:\n",
    "    convert_with_tika(filen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_clear = open(\"test-tei/7.tei.xml\",\"r\")\n",
    "text = file_to_clear.read()\n",
    "text = re.sub(r\"(<ref.*>)|(<.[^(><.)]+>)\",\"\",text)\n",
    "file_writer = open(\"output_text/grobid_7.tei.xml.txt\",\"a+\",encoding=\"utf-8\")\n",
    "file_writer.write(text)\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 158.38 MiB, increment: 0.03 MiB\n",
      "grobid_2 ,NLTK, 23 ,Spacy, 22 ,Gensim, 129\n",
      "grobid_7 ,NLTK, 33 ,Spacy, 35 ,Gensim, 94\n",
      "grobid_4 ,NLTK, 400 ,Spacy, 398 ,Gensim, 410\n",
      "grobid_1 ,NLTK, 163 ,Spacy, 157 ,Gensim, 162\n",
      "grobid_8 ,NLTK, 192 ,Spacy, 192 ,Gensim, 193\n",
      "grobid_10 ,NLTK, 78 ,Spacy, 78 ,Gensim, 78\n",
      "grobid_6 ,NLTK, 90 ,Spacy, 90 ,Gensim, 91\n",
      "grobid_3 ,NLTK, 114 ,Spacy, 113 ,Gensim, 124\n",
      "grobid_9 ,NLTK, 147 ,Spacy, 146 ,Gensim, 155\n",
      "grobid_5 ,NLTK, 33 ,Spacy, 32 ,Gensim, 70\n",
      "CPU times: user 644 ms, sys: 173 ms, total: 818 ms\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%reload_ext memory_profiler\n",
    "%memit\n",
    "file_path = \"output_text/grobid\"\n",
    "files_list = get_filenames(\"txt\")\n",
    "for filen in files_list:\n",
    "    file_handle = file_path+\"/\"+filen\n",
    "    f_reader = open(file_handle,\"r\",encoding=\"utf-8\")\n",
    "    data = f_reader.read()\n",
    "    sent_list_nltk = get_sent_by_nltk(data)\n",
    "    sent_list_spacy = get_sent_by_spacy(data)\n",
    "    sent_list_gensim = get_sent_by_gensim(data)\n",
    "    print(filen.split(\".\")[0],\",NLTK,\",len(sent_list_nltk),\",Spacy,\",len(sent_list_spacy),\",Gensim,\",len(sent_list_gensim))\n",
    "    # print(\"Spacy Count:\",len(sent_list_spacy))\n",
    "    # print(\"Gensim Count:\",len(sent_list_gensim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}